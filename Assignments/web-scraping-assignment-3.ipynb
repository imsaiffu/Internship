{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73aabd29",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1289f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product name to search on Amazon: guitar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    search_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Example usage\n",
    "product_name = input(\"Enter the product name to search on Amazon: \")\n",
    "soup = search_amazon(product_name)\n",
    "\n",
    "# Display the titles of the products in the search results\n",
    "products = soup.find_all('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "for idx, product in enumerate(products, 1):\n",
    "    print(f\"{idx}. {product.get_text(strip=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c554",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcf1998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to guitar_amazon_products.csv\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_product_details(soup):\n",
    "    product_list = []\n",
    "    \n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    for product in products:\n",
    "        title = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "        price = product.find('span', class_='a-price-whole')\n",
    "        url = product.find('a', class_='a-link-normal a-text-normal')['href']\n",
    "        product_url = f\"https://www.amazon.in{url}\"\n",
    "        try:\n",
    "            brand = product.find('span', class_='a-size-base-plus a-color-base').get_text(strip=True)\n",
    "        except:\n",
    "            brand = \"-\"\n",
    "        \n",
    "        product_details = {\n",
    "            'Brand Name': brand if brand else \"-\",\n",
    "            'Name of the Product': title.get_text(strip=True) if title else \"-\",\n",
    "            'Price': price.get_text(strip=True) if price else \"-\",\n",
    "            'Return/Exchange': \"-\",\n",
    "            'Expected Delivery': \"-\",\n",
    "            'Availability': \"-\",\n",
    "            'Product URL': product_url\n",
    "        }\n",
    "        product_list.append(product_details)\n",
    "    \n",
    "    return product_list\n",
    "\n",
    "def scrape_amazon(product_name, pages=3):\n",
    "    all_products = []\n",
    "    for page in range(1, pages + 1):\n",
    "        search_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}&page={page}\"\n",
    "        response = requests.get(search_url, 'headers=headers')\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = get_product_details(soup)\n",
    "        all_products.extend(products)\n",
    "    return all_products\n",
    "\n",
    "# Scrape details for the first 3 pages of search results\n",
    "product_details = scrape_amazon(product_name, pages=3)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(product_details)\n",
    "df.to_csv(f\"{product_name}_amazon_products.csv\", index=False)\n",
    "print(f\"Scraped data saved to {product_name}_amazon_products.csv\")\n",
    "\n",
    "# Example usage\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe7308",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf1c34a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     50\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfruits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuitar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCakes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m scrape_google_images(keywords)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Display the image URLs\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword, urls \u001b[38;5;129;01min\u001b[39;00m image_urls\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mscrape_google_images\u001b[1;34m(keywords, num_images)\u001b[0m\n\u001b[0;32m     13\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--no-sandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--disable-dev-shm-usage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall()), options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     17\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\"from webdriver_manager.chrome import ChromeDriverManager\"\n",
    "import time\n",
    "\n",
    "def scrape_google_images(keywords, num_images=10):\n",
    "    # Set up the Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    image_urls = {}\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        driver.get(\"https://images.google.com/\")\n",
    "        search_box = driver.find_element(By.NAME, \"q\")\n",
    "        search_box.send_keys(keyword)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)  # Allow time for the search results to load\n",
    "        \n",
    "        image_urls[keyword] = []\n",
    "        thumbnails = driver.find_elements(By.CSS_SELECTOR, \"img.Q4LuWd\")\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            try:\n",
    "                thumbnails[i].click()\n",
    "                time.sleep(2)  # Allow time for the full image to load\n",
    "                images = driver.find_elements(By.CSS_SELECTOR, \"img.n3VNCb\")\n",
    "                for image in images:\n",
    "                    src = image.get_attribute(\"src\")\n",
    "                    if src and \"http\" in src:\n",
    "                        image_urls[keyword].append(src)\n",
    "                        if len(image_urls[keyword]) >= num_images:\n",
    "                            break\n",
    "                if len(image_urls[keyword]) >= num_images:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching image {i+1} for {keyword}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    driver.quit()\n",
    "    return image_urls\n",
    "\n",
    "# Example usage\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "image_urls = scrape_google_images(keywords)\n",
    "\n",
    "# Display the image URLs\n",
    "for keyword, urls in image_urls.items():\n",
    "    print(f\"\\nImages for '{keyword}':\")\n",
    "    for url in urls:\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55bc152",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d4807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to Oneplus Nord_flipkart_products.csv\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart(smartphone_name):\n",
    "    search_url = f\"https://www.flipkart.com/search?q={smartphone_name.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    \n",
    "    product_list = []\n",
    "    \n",
    "    for product in products:\n",
    "        try:\n",
    "            name = product.find('div', {'class': '_4rR01T'}).get_text(strip=True)\n",
    "            link = \"https://www.flipkart.com\" + product.find('a', {'class': '_1fQZEK'})['href']\n",
    "            price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).get_text(strip=True)\n",
    "            details = product.find_all('li', {'class': 'rgWa7D'})\n",
    "            details_text = [detail.get_text(strip=True) for detail in details]\n",
    "            \n",
    "            product_details = {\n",
    "                'Brand Name': name.split(' ')[0],\n",
    "                'Smartphone name': name,\n",
    "                'Colour': details_text[0] if len(details_text) > 0 else \"-\",\n",
    "                'RAM': details_text[1] if len(details_text) > 1 else \"-\",\n",
    "                'Storage(ROM)': details_text[2] if len(details_text) > 2 else \"-\",\n",
    "                'Primary Camera': details_text[3] if len(details_text) > 3 else \"-\",\n",
    "                'Secondary Camera': details_text[4] if len(details_text) > 4 else \"-\",\n",
    "                'Display Size': details_text[5] if len(details_text) > 5 else \"-\",\n",
    "                'Battery Capacity': details_text[6] if len(details_text) > 6 else \"-\",\n",
    "                'Price': price,\n",
    "                'Product URL': link\n",
    "            }\n",
    "            product_list.append(product_details)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return product_list\n",
    "\n",
    "# Example usage\n",
    "smartphone_name = \"Oneplus Nord\"\n",
    "product_details = scrape_flipkart(smartphone_name)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(product_details)\n",
    "df.to_csv(f\"{smartphone_name}_flipkart_products.csv\", index=False)\n",
    "print(f\"Scraped data saved to {smartphone_name}_flipkart_products.csv\")\n",
    "\n",
    "# Display some scraped data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e308b",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a94115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    # Set up the Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "    search_box = driver.find_element(By.ID, \"searchboxinput\")\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Allow time for the search results to load\n",
    "    \n",
    "    current_url = driver.current_url\n",
    "    driver.quit()\n",
    "    \n",
    "    try:\n",
    "        lat_long = current_url.split('@')[1].split(',')[:2]\n",
    "        latitude = lat_long[0]\n",
    "        longitude = lat_long[1]\n",
    "    except Exception as e:\n",
    "        latitude, longitude = \"-\", \"-\"\n",
    "    \n",
    "    return latitude, longitude\n",
    "\n",
    "# Example usage\n",
    "city_name = \"London\"\n",
    "latitude, longitude = get_coordinates(city_name)\n",
    "print(f\"Coordinates of {city_name}: Latitude = {latitude}, Longitude = {longitude}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0982aa",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_digit_best_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    laptops = soup.find_all('div', {'class': 'TopNumbeHeading sticky-footer'})\n",
    "    \n",
    "    laptop_list = []\n",
    "    \n",
    "    for laptop in laptops:\n",
    "        try:\n",
    "            name = laptop.find('h2').get_text(strip=True)\n",
    "            details = laptop.find_all('li')\n",
    "            details_text = [detail.get_text(strip=True) for detail in details]\n",
    "            laptop_details = {\n",
    "                'Name': name,\n",
    "                'Details': details_text\n",
    "            }\n",
    "            laptop_list.append(laptop_details)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return laptop_list\n",
    "\n",
    "# Example usage\n",
    "laptop_details = scrape_digit_best_gaming_laptops()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(laptop_details)\n",
    "df.to_csv(\"best_gaming_laptops_digit.csv\", index=False)\n",
    "print(\"Scraped data saved to best_gaming_laptops_digit.csv\")\n",
    "\n",
    "# Display some scraped data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a9f90",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ecf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    billionaires = soup.find_all('div', {'class': 'personName'})\n",
    "    \n",
    "    billionaire_list = []\n",
    "    \n",
    "    for billionaire in billionaires:\n",
    "        try:\n",
    "            rank = billionaire.find('div', {'class': 'rank'}).get_text(strip=True)\n",
    "            name = billionaire.find('div', {'class': 'name'}).get_text(strip=True)\n",
    "            net_worth = billionaire.find('div', {'class': 'netWorth'}).get_text(strip=True)\n",
    "            age = billionaire.find('div', {'class': 'age'}).get_text(strip=True)\n",
    "            citizenship = billionaire.find('div', {'class': 'country'}).get_text(strip=True)\n",
    "            source = billionaire.find('div', {'class': 'source'}).get_text(strip=True)\n",
    "            industry = billionaire.find('div', {'class': 'category'}).get_text(strip=True)\n",
    "            billionaire_details = {\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            }\n",
    "            billionaire_list.append(billionaire_details)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return billionaire_list\n",
    "\n",
    "# Example usage\n",
    "billionaire_details = scrape_forbes_billionaires()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(billionaire_details)\n",
    "df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "print(\"Scraped data saved to forbes_billionaires.csv\")\n",
    "\n",
    "# Display some scraped data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f45bd0",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e58ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def scrape_youtube_comments(video_url, num_comments=500):\n",
    "    # Set up the Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    driver.get(video_url)\n",
    "    time.sleep(5)  # Allow time for the video page to load\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    comments = []\n",
    "    while len(comments) < num_comments:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        comment_elements = driver.find_elements(By.CSS_SELECTOR, \"#content-text\")\n",
    "        for element in comment_elements:\n",
    "            try:\n",
    "                comment_text = element.text\n",
    "                comment_upvotes = element.find_element(By.CSS_SELECTOR, \"#vote-count-middle\").text\n",
    "                comment_time = element.find_element(By.CSS_SELECTOR, \"#header-author > yt-formatted-string > a\").text\n",
    "                comment_details = {\n",
    "                    'Comment': comment_text,\n",
    "                    'Upvotes': comment_upvotes,\n",
    "                    'Time': comment_time\n",
    "                }\n",
    "                if len(comments) < num_comments:\n",
    "                    comments.append(comment_details)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    driver.quit()\n",
    "    return comments\n",
    "\n",
    "# Example usage\n",
    "video_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "comments = scrape_youtube_comments(video_url, num_comments=500)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(comments)\n",
    "df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "print(\"Scraped data saved to youtube_comments.csv\")\n",
    "\n",
    "# Display some scraped data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857fe38",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hostelworld(location):\n",
    "    search_url = f\"https://www.hostelworld.com/search?search_keywords={location.replace(' ', '%20')}&country=England\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    hostels = soup.find_all('div', {'class': 'property-card'})\n",
    "    \n",
    "    hostel_list = []\n",
    "    \n",
    "    for hostel in hostels:\n",
    "        try:\n",
    "            name = hostel.find('h2', {'class': 'title'}).get_text(strip=True)\n",
    "            distance = hostel.find('span', {'class': 'distance'}).get_text(strip=True)\n",
    "            rating = hostel.find('div', {'class': 'score'}).get_text(strip=True)\n",
    "            reviews = hostel.find('span', {'class': 'reviews'}).get_text(strip=True)\n",
    "            overall = hostel.find('div', {'class': 'keyword'}).get_text(strip=True)\n",
    "            privates_price = hostel.find('a', {'class': 'prices'}).get_text(strip=True)\n",
    "            dorms_price = hostel.find('a', {'class': 'prices'}).get_text(strip=True)\n",
    "            facilities = hostel.find('div', {'class': 'facilities'}).get_text(strip=True)\n",
    "            description = hostel.find('div', {'class': 'desc'}).get_text(strip=True)\n",
    "            hostel_details = {\n",
    "                'Hostel Name': name,\n",
    "                'Distance from City Centre': distance,\n",
    "                'Rating': rating,\n",
    "                'Total Reviews': reviews,\n",
    "                'Overall Reviews': overall,\n",
    "                'Privates from Price': privates_price,\n",
    "                'Dorms from Price': dorms_price,\n",
    "                'Facilities': facilities,\n",
    "                'Property Description': description\n",
    "            }\n",
    "            hostel_list.append(hostel_details)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return hostel_list\n",
    "\n",
    "# Example usage\n",
    "location = \"London\"\n",
    "hostel_details = scrape_hostelworld(location)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(hostel_details)\n",
    "df.to_csv(f\"hostels_in_{location}.csv\", index=False)\n",
    "print(f\"Scraped data saved to hostels_in_{location}.csv\")\n",
    "\n",
    "# Display some scraped data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1206ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc9a9a6",
   "metadata": {},
   "source": [
    "- Summary\n",
    "- Q1 & Q2: Scrape Amazon product listings based on a user input and save to CSV.\n",
    "- Q3: Scrape image URLs from Google Images for specified keywords.\n",
    "- Q4: Scrape smartphone details from Flipkart's first search results page.\n",
    "- Q5: Scrape geospatial coordinates of a city from Google Maps.\n",
    "- Q6: Scrape best gaming laptops details from Digit.in.\n",
    "- Q7: Scrape billionaire details from Forbes.\n",
    "- Q8: Scrape YouTube comments and save to CSV.\n",
    "- Q9: Scrape hostel details from Hostelworld for a specified location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35568f17",
   "metadata": {},
   "source": [
    "These solutions utilize a combination of requests, BeautifulSoup, selenium, and pandas to automate the web scraping tasks and save the results in a structured format like CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d84b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
